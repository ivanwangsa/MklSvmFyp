\documentclass[11pt]{article}%Don't change
\usepackage{pstricks,pstricks-add,pst-math,pst-xkey}
\usepackage{amsfonts,amsmath,amsthm,amssymb,amsrefs,array,dcolumn,epsfig,empheq}%Don't change

%\pagestyle{fancy}
%\fancyhf{}
%\rhead{Ivan Wangsaciptalingga}
%\lhead{Volatility Index Futures}
%\cfoot{Page \thepage}

\def\nn{\mathbb{N}}
\def\rr{\mathbb{R}}
\def\zz{\mathbb{Z}}
\def\qq{\mathbb{Q}}
\def\pp{\mathbb{P}}
\def\cc{\mathbb{C}}
\def\ss{\mathbb{S}}

\def\cl{{\bf\noindent\par Claim:  }\nopagebreak}
\def\soln{{\bf\noindent\par Solution.  }\nopagebreak}
\def\sol1{{\bf\noindent\par First Solution.  }\nopagebreak}
\def\sol2{{\bf\noindent\par Second Solution.  }\nopagebreak}
\def\pf{{\bf\noindent\par Proof:  }\nopagebreak}
\def\ds{\displaystyle}
\def\bs{\boldsymbol}

\begin{document}

\section{Single-kernel Support Vector Machines}

Support vector machines are based on the idea of a \textit{fat margin}; consider a set of points divided into 2 classes which are linearly separable, a support vector machine will figure out the hyperplane that separates the two classes with the fattest margin; the fatter, the better. We may formalize the idea as follows

\begin{center}
\begin{tabular}{rll}
minimise & \multicolumn{2}{l}{$\ds \frac12 ||\bs w||^2$} \\
with respect to & \multicolumn{2}{l}{$\bs w, b$} \\
subject to & $y_i(\bs w\cdot \bs x_i+b) \geq 1 $ & $\forall i=1,\ldots,m$
\end{tabular}
\end{center}

where $m$ is the number of observations, $\bs x_i$ are the observations and $y_i$ are the target value for each $i$. Graphically, this is what it means:

[insert picture]

This problem has an associated dual problem, which are as follows:

\begin{center}
\begin{tabular}{rll}
maximise & $\ds \sum_{i=1}^m\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_j \bs x_i\cdot \bs x_j$ \\
with respect to & $\bs\alpha$ \\
subject to & $\bs\alpha\succeq\bs 0$ \\
 & $\bs\alpha^T\textbf{y}=0$
\end{tabular}
\end{center}

The strength of support vector machine lies on the fact that we can map each point $\bs x_i$ to a point on another space with different (and possibly larger) dimensionality; this space is called the feature space. We can transform dot products on the $\bs x_i$'s to the following
\[\bs x_i\cdot\bs x_j \rightarrow \phi(\bs x_i)\cdot\phi(\bs x_j)\]
where $\phi(.)$ is the mapping function onto the feature space. As it turns out, we don't need to specify the mapping function explicitly; what we need to do is to specify the so-called \textit{kernel function}:
\[K(\bs x_i, \bs x_j) = \phi(\bs x_i)\cdot\phi(\bs x_j)\]

Thus we can reformulate those problems that we'd had entirely, with a slight modification. The following is the primal problem

\begin{center}
\begin{tabular}{rll}
minimise & \multicolumn{2}{l}{$\ds \frac12 ||\bs w||^2$} \\
with respect to & \multicolumn{2}{l}{$\bs w, b$} \\
subject to & $y_i(\bs w\cdot \phi(\bs x_i)+b) \geq 1 $ & $\forall i=1,\ldots,m$
\end{tabular}
\end{center}

and this is the dual problem

\begin{center}
\begin{tabular}{rll}
maximise & $\ds \sum_{i=1}^m\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jK(\bs x_i,\bs x_j)$ \\
with respect to & $\bs\alpha$ \\
subject to & $\bs\alpha\succeq\bs 0$ \\
 & $\bs\alpha^T\textbf{y}=0$
\end{tabular}
\end{center}

There are also several kernels that may be used:

\begin{itemize}
\item Linear kernel: $K(\bs x_i, \bs x_j) = \bs x_i \cdot \bs x_j$
\item Polynomial kernel: $K(\bs x_i, \bs x_j) = (1 + \bs x_i \cdot \bs x_j)^d$
\item Gaussian kernel: $\ds K(\bs x_i, \bs x_j) = \exp\left(-\frac{||\bs x_i-\bs x_j||^2}{2\sigma^2}\right)$
\item Sigmoid kernel: $\ds K(\bs x_i, \bs x_j) = \tanh(\beta \bs x_i \cdot \bs x_j + b)$
\end{itemize}

Not all functions can be kernels; it must satisfy Mercer's condition. Several kernels that don't satisfy Mercer's condition can in fact generalize well; the sigmoid kernel is one example.

Solving this dual problem will gives us the solution of the primal problem as well, due to the strong duality nature of the problem. Given the solution to dual problem $\bs\alpha^*$, which may be obtained from quadratic optimization solver, we may compute the bias

\[b = -\frac12\left[\max_{\{i|y_i=-1\}}\left(\sum_{j=1}^m \alpha_jy_jK(\bs x_i, \bs x_j)\right) - \min_{\{i|y_i=+1\}}\left(\sum_{j=1}^m\alpha_jy_jK(\bs x_i, \bs x_j)\right)\right]\]

and the predicted class of a new data point $\bs z$, which is based on the sign of the following

\[\phi(\bs z) = b^* + \sum_{i=1}^m \alpha_i^*y_iK(\bs x_i, \bs z).\]

These seems computationally expensive, especially as we move on to larger datasets, but as it turns out $\bs \alpha^*$ is a sparse vector; there are a lot of indices $i$ such that $\alpha_i = 0$. For any $i$ such that $\alpha_i\neq 0$, we define the vector $\bs x_i$ to be a support vector. Thus the complexity of the computation reduces to the order on the number of support vectors.

Now let us consider the generalization ability of an SVM. Letting $R$ to be the maximum radius of which a ball may contain all the datasets, and $\gamma$ to be the fattest margin, we obtain the generalization inequality on SVM, which was proven by Vapnik in Statistical Learning Theory
\[\text{Generalization error} \leq \frac{R^2}{m\gamma^2}\]
thus even though the VC dimension of an SVM may be infinite (for instance, consider SVM with Gaussian kernel of small $\sigma$), we may still generalize well due to this inequality.

\subsection{Soft Margin SVM}

We may in fact modify SVM to allow for non-separable datasets. There are two most popular modifications, namely the $\ell_1$-norm and the $\ell_2$-norm regularisation.

For the $\ell_1$-norm, we have the following primal problem

\begin{center}
\begin{tabular}{rll}
minimise & \multicolumn{2}{l}{$\ds \frac12 ||\bs w||^2 + C\sum_{i=1}^m \xi_i$} \\
with respect to & \multicolumn{2}{l}{$\bs w, \bs \xi, b$} \\
subject to & $y_i(\bs w\cdot \phi(\bs x_i)+b) \geq 1 - \xi_i$ & $\forall i=1,\ldots,m$
\end{tabular}
\end{center}

with the dual

\begin{center}
\begin{tabular}{rll}
maximise & $\ds \sum_{i=1}^m\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jK(\bs x_i,\bs x_j)$ \\
with respect to & $\bs\alpha$ \\
subject to & $\bs 0 \preceq\bs\alpha\preceq C\bs 1$ \\
 & $\bs\alpha^T\textbf{y}=0$
\end{tabular}
\end{center}

again, this is solvable easily by a QP solver.

For $\ell_2$-norm, we have the following primal problem

\begin{center}
\begin{tabular}{rll}
minimise & \multicolumn{2}{l}{$\ds \frac12 ||\bs w||^2 + C\sum_{i=1}^m \xi_i^2$} \\
with respect to & \multicolumn{2}{l}{$\bs w, \bs \xi, b$} \\
subject to & $y_i(\bs w\cdot \phi(\bs x_i)+b) \geq 1 - \xi_i$ & $\forall i=1,\ldots,m$
\end{tabular}
\end{center}

with the dual problem

\begin{center}
\begin{tabular}{rll}
maximise & $\ds \sum_{i=1}^m\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jK(\bs x_i,\bs x_j)-\frac{1}{4C}\sum_{i=1}^m\alpha_i^2$ \\
with respect to & $\bs\alpha$ \\
subject to & $\bs 0 \preceq\bs\alpha\preceq C\bs 1$ \\
 & $\bs\alpha^T\textbf{y}=0$
\end{tabular}
\end{center}

which is basically the same as the hard-margin problem, except with the modified Gram matrix of $K(\bs x_i, \bs x_i) \leftarrow K(\bs x_i, \bs x_i) + \frac{1}{2C}$.

\subsection{More on Kernels}

Some of the kernels that we'd had so far have parameter(s) that should be decided before the SVM gives us the result. This brings us to a question: how do we determine these parameters? Several methods exist:

\begin{itemize}
\item Separate the data set into training set, validation set, and test set. For each choice of parameters we have, we train the machine using the training set, and find the error on validation set; this is called the validation error. Then we pick the parameters that produces the smallest validation error.
\item Use cross validation; the most famous example is the leave-one-out. In this procedure, for each data point, we extract that data point out of the training set, and train the machine with the rest of $(m-1)$ points. We evaluate the hypothesis versus the singled-out point; the expected value of the error is known as the cross-validation error. We don't need to single out one point; in fact $k$-fold cross validation is common, when we take $k$ points out of the training sets. Practitioners recommend the size of $k$ to be between $m/5$ and $m/10$.
\item Minimise the generalization bound; by varying the parameters and training the machine, we may obtain the formula for $R$ and $\gamma$. They are as follows:
\[R^2 = \max_i\left\{\sum_{j=1}^m\sum_{k=1}^m \lambda_j\lambda_kK(\bs x_j, \bs x_k) - 2\sum_{j=1}^m\lambda_jK(\bs x_i, \bs x_j)+K(\bs x_i,\bs x_i)\right\}\]
\[\frac{1}{\gamma^2}=\sum_{i=1}^m\alpha_i^*\]
Thus we can minimize them as the parameters vary, and choose the parameters with the smallest generalization error bound.
\item Use multiple kernel learning; for instance, use a linear combination of chosen kernels, and solve the optimization problem as we allow the weights to vary. This is the purpose of our final year project.
\end{itemize}

\section{Multiple Kernel Learning}
Now consider a set of kernels $\mathcal{K}$. For each kernel $K\in\mathcal{K}$, we may define the following function
\[\omega(K) = \max_{\bs \alpha}\left\{\sum_{i=1}^m\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m \alpha_i \alpha_j y_i y_j K(\bs x_i, \bs x_j) : \bs\alpha^T\bs y=0, \bs\alpha\succeq \bs 0\right\}.\]
Now by stating the constraint explicitly and assuming that the dataset is linearly separable on the feature space, we have the following problem 

\begin{center}
\begin{tabular}{rll}
\multicolumn{3}{l}{$\ds \min_{K\in\mathcal{K}} \omega(K) = \min_{K\in\mathcal{K}}\max_{\bs \alpha} \sum_{i=1}^m\alpha_i-\frac12\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jK(\bs x_i,\bs x_j)$} \\
with respect to & $K$, $\bs\alpha$ \\
subject to & $\bs 0 \preceq\bs\alpha\preceq C\bs 1$ \\
 & $\bs\alpha^T\textbf{y}=0$
\end{tabular}
\end{center}
\end{document}